{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98f687e",
   "metadata": {},
   "source": [
    "# MovieLens-100k Data Preprocessing for P5\n",
    "\n",
    "This notebook preprocesses MovieLens-100k dataset for P5 training.\n",
    "\n",
    "## Download the dataset:\n",
    "```bash\n",
    "cd raw_data\n",
    "wget https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip ml-100k.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5ff5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(r\"C:\\Users\\lehoa\\OneDrive\\Documents\\College\\Lab\\Code\\P5\")\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Set seeds\n",
    "seed = 2020\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d24cbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "short_data_name = 'ml100k'\n",
    "os.makedirs(short_data_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb40fc",
   "metadata": {},
   "source": [
    "## Load MovieLens-100k Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c21e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movielens_ratings(rating_score=3.0):\n",
    "    \"\"\"\n",
    "    Load MovieLens-100k ratings data\n",
    "    Format: user_id \\t item_id \\t rating \\t timestamp\n",
    "    \"\"\"\n",
    "    datas = []\n",
    "    data_file = './raw_data/ml-100k/u.data'\n",
    "    \n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            user = parts[0]\n",
    "            item = parts[1]\n",
    "            rating = float(parts[2])\n",
    "            timestamp = int(parts[3])\n",
    "            \n",
    "            if rating <= rating_score:  # Filter low ratings\n",
    "                continue\n",
    "            \n",
    "            datas.append((user, item, timestamp))\n",
    "    \n",
    "    return datas\n",
    "\n",
    "def load_movielens_meta():\n",
    "    \"\"\"\n",
    "    Load MovieLens-100k item metadata\n",
    "    Format: movie_id | movie_title | release_date | video_release_date | IMDb_URL | genres\n",
    "    \"\"\"\n",
    "    meta_file = './raw_data/ml-100k/u.item'\n",
    "    meta_data = {}\n",
    "    \n",
    "    genre_names = ['unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "                   'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "    \n",
    "    with open(meta_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('|')\n",
    "            item_id = parts[0]\n",
    "            title = parts[1]\n",
    "            \n",
    "            # Get genres (last 19 columns are binary genre indicators)\n",
    "            genres = []\n",
    "            for i, is_genre in enumerate(parts[5:]):\n",
    "                if is_genre == '1':\n",
    "                    genres.append(genre_names[i])\n",
    "            \n",
    "            meta_data[item_id] = {\n",
    "                'title': title,\n",
    "                'categories': [genres] if genres else [['unknown']]\n",
    "            }\n",
    "    \n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344f9f7",
   "metadata": {},
   "source": [
    "## Core Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99772a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction(datas):\n",
    "    \"\"\"Convert raw data to user sequences\"\"\"\n",
    "    user_seq = {}\n",
    "    for data in datas:\n",
    "        user, item, time = data\n",
    "        if user in user_seq:\n",
    "            user_seq[user].append((item, time))\n",
    "        else:\n",
    "            user_seq[user] = [(item, time)]\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    for user, item_time in user_seq.items():\n",
    "        item_time.sort(key=lambda x: x[1])\n",
    "        items = [t[0] for t in item_time]\n",
    "        user_seq[user] = items\n",
    "    \n",
    "    return user_seq\n",
    "\n",
    "def check_Kcore(user_items, user_core, item_core):\n",
    "    \"\"\"Check if data satisfies K-core property\"\"\"\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            user_count[user] += 1\n",
    "            item_count[item] += 1\n",
    "    \n",
    "    for user, num in user_count.items():\n",
    "        if num < user_core:\n",
    "            return user_count, item_count, False\n",
    "    for item, num in item_count.items():\n",
    "        if num < item_core:\n",
    "            return user_count, item_count, False\n",
    "    return user_count, item_count, True\n",
    "\n",
    "def filter_Kcore(user_items, user_core, item_core):\n",
    "    \"\"\"Iteratively filter data to satisfy K-core\"\"\"\n",
    "    user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    while not isKcore:\n",
    "        for user, num in list(user_count.items()):\n",
    "            if user_count[user] < user_core:\n",
    "                user_items.pop(user, None)\n",
    "            else:\n",
    "                user_items[user] = [item for item in user_items[user] if item_count[item] >= item_core]\n",
    "        user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    return user_items\n",
    "\n",
    "def id_map(user_items):\n",
    "    \"\"\"Create ID mappings for users and items\"\"\"\n",
    "    user2id = {}\n",
    "    item2id = {}\n",
    "    id2user = {}\n",
    "    id2item = {}\n",
    "    user_id = 1\n",
    "    item_id = 1\n",
    "    final_data = {}\n",
    "    \n",
    "    random_user_list = list(user_items.keys())\n",
    "    random.shuffle(random_user_list)\n",
    "    \n",
    "    for user in random_user_list:\n",
    "        items = user_items[user]\n",
    "        if user not in user2id:\n",
    "            user2id[user] = str(user_id)\n",
    "            id2user[str(user_id)] = user\n",
    "            user_id += 1\n",
    "        \n",
    "        iids = []\n",
    "        for item in items:\n",
    "            if item not in item2id:\n",
    "                item2id[item] = str(item_id)\n",
    "                id2item[str(item_id)] = item\n",
    "                item_id += 1\n",
    "            iids.append(item2id[item])\n",
    "        \n",
    "        uid = user2id[user]\n",
    "        final_data[uid] = iids\n",
    "    \n",
    "    data_maps = {\n",
    "        'user2id': user2id,\n",
    "        'item2id': item2id,\n",
    "        'id2user': id2user,\n",
    "        'id2item': id2item\n",
    "    }\n",
    "    return final_data, user_id-1, item_id-1, data_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c900a60",
   "metadata": {},
   "source": [
    "## Process MovieLens Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "816369d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieLens-100k raw data loaded! Total interactions: 100000\n",
      "Converted to user sequences. Users: 943\n",
      "User 5-core and Item 5-core complete!\n",
      "Total User: 943, Avg User: 105.2884, Min Len: 19, Max Len: 648\n",
      "Total Item: 1349, Avg Item: 73.6004, Min Inter: 5, Max Inter: 583\n",
      "Interaction Num: 99287, Sparsity: 92.20%\n"
     ]
    }
   ],
   "source": [
    "rating_score = 0.0  # Filter threshold\n",
    "user_core = 5\n",
    "item_core = 5\n",
    "\n",
    "# Load data\n",
    "datas = load_movielens_ratings(rating_score=rating_score)\n",
    "print(f'MovieLens-100k raw data loaded! Total interactions: {len(datas)}')\n",
    "\n",
    "# Get user-item interactions\n",
    "user_items = get_interaction(datas)\n",
    "print(f'Converted to user sequences. Users: {len(user_items)}')\n",
    "\n",
    "# Apply K-core filtering\n",
    "user_items = filter_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "print(f'User {user_core}-core and Item {item_core}-core complete!')\n",
    "\n",
    "# Create ID mappings\n",
    "user_items, user_num, item_num, data_maps = id_map(user_items)\n",
    "user_count, item_count, _ = check_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "\n",
    "# Statistics\n",
    "user_count_list = list(user_count.values())\n",
    "user_avg, user_min, user_max = np.mean(user_count_list), np.min(user_count_list), np.max(user_count_list)\n",
    "item_count_list = list(item_count.values())\n",
    "item_avg, item_min, item_max = np.mean(item_count_list), np.min(item_count_list), np.max(item_count_list)\n",
    "interact_num = np.sum(user_count_list)\n",
    "sparsity = (1 - interact_num / (user_num * item_num)) * 100\n",
    "\n",
    "print(f'Total User: {user_num}, Avg User: {user_avg:.4f}, Min Len: {user_min}, Max Len: {user_max}')\n",
    "print(f'Total Item: {item_num}, Avg Item: {item_avg:.4f}, Min Inter: {item_min}, Max Inter: {item_max}')\n",
    "print(f'Interaction Num: {interact_num}, Sparsity: {sparsity:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c572b9",
   "metadata": {},
   "source": [
    "## Process Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8be49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 1682 items\n",
      "Filtered metadata to 1349 items in dataset\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "raw_meta = load_movielens_meta()\n",
    "print(f'Loaded metadata for {len(raw_meta)} items')\n",
    "\n",
    "# Filter metadata to only include items in our dataset\n",
    "meta_data = {}\n",
    "for raw_item_id, info in raw_meta.items():\n",
    "    if raw_item_id in data_maps['item2id']:\n",
    "        item_id = data_maps['item2id'][raw_item_id]\n",
    "        meta_data[item_id] = info\n",
    "\n",
    "print(f'Filtered metadata to {len(meta_data)} items in dataset')\n",
    "\n",
    "# Save as gzipped JSON (following P5 format)\n",
    "import gzip\n",
    "meta_file = f'./{short_data_name}/meta.json.gz'\n",
    "with gzip.open(meta_file, 'wt', encoding='utf-8') as f:\n",
    "    json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1077a0",
   "metadata": {},
   "source": [
    "## Save Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be6c745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sequential_data.txt and datamaps.json to ml100k/\n"
     ]
    }
   ],
   "source": [
    "# Save sequential data\n",
    "data_file = f'./{short_data_name}/sequential_data.txt'\n",
    "with open(data_file, 'w') as out:\n",
    "    for user, items in user_items.items():\n",
    "        out.write(user + ' ' + ' '.join(items) + '\\n')\n",
    "\n",
    "# Save datamaps\n",
    "datamaps_file = f'./{short_data_name}/datamaps.json'\n",
    "with open(datamaps_file, 'w') as out:\n",
    "    json.dump(data_maps, out)\n",
    "\n",
    "print(f'Saved sequential_data.txt and datamaps.json to {short_data_name}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d4d74",
   "metadata": {},
   "source": [
    "## Generate Negative Samples for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d71fee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative_samples.txt with 943 users\n"
     ]
    }
   ],
   "source": [
    "def sample_test_data(user_items, test_num=99, sample_type='random'):\n",
    "    \"\"\"\n",
    "    Sample negative items for each user for testing\n",
    "    sample_type: 'random' or 'pop'\n",
    "    \"\"\"\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            item_count[int(item)] += 1\n",
    "    \n",
    "    all_item = list(item_count.keys())\n",
    "    count = list(item_count.values())\n",
    "    sum_value = np.sum(count)\n",
    "    probability = [value / sum_value for value in count]\n",
    "    \n",
    "    user_neg_items = {}\n",
    "    \n",
    "    for user, user_seq in user_items.items():\n",
    "        user_seq_int = [int(i) for i in user_seq]\n",
    "        test_samples = []\n",
    "        while len(test_samples) < test_num:\n",
    "            if sample_type == 'random':\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False)\n",
    "            else:\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False, p=probability)\n",
    "            sample_ids = [str(item) for item in sample_ids if item not in user_seq_int and str(item) not in test_samples]\n",
    "            test_samples.extend(sample_ids)\n",
    "        test_samples = test_samples[:test_num]\n",
    "        user_neg_items[user] = test_samples\n",
    "    \n",
    "    return user_neg_items\n",
    "\n",
    "# Generate negative samples\n",
    "user_neg_items = sample_test_data(user_items)\n",
    "\n",
    "# Save negative samples\n",
    "test_file = f'./{short_data_name}/negative_samples.txt'\n",
    "with open(test_file, 'w') as out:\n",
    "    for user, samples in user_neg_items.items():\n",
    "        out.write(user + ' ' + ' '.join(samples) + '\\n')\n",
    "\n",
    "print(f'Saved negative_samples.txt with {len(user_neg_items)} users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4192c",
   "metadata": {},
   "source": [
    "## Create Rating Splits\n",
    "\n",
    "For rating prediction task, we need to create train/val/test splits from the ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f491a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99287 rating records\n"
     ]
    }
   ],
   "source": [
    "def load_all_ratings():\n",
    "    \"\"\"Load all ratings (not filtered by score)\"\"\"\n",
    "    rating_data = []\n",
    "    data_file = './raw_data/ml-100k/u.data'\n",
    "    \n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            user = parts[0]\n",
    "            item = parts[1]\n",
    "            rating = float(parts[2])\n",
    "            timestamp = int(parts[3])\n",
    "            \n",
    "            # Only include users and items that are in our filtered dataset\n",
    "            if user in data_maps['user2id'] and item in data_maps['item2id']:\n",
    "                user_id = data_maps['user2id'][user]\n",
    "                item_id = data_maps['item2id'][item]\n",
    "                \n",
    "                rating_data.append({\n",
    "                    'user': user_id,\n",
    "                    'item': item_id,\n",
    "                    'rating': rating,\n",
    "                    'timestamp': timestamp\n",
    "                })\n",
    "    \n",
    "    return rating_data\n",
    "\n",
    "rating_data = load_all_ratings()\n",
    "print(f'Loaded {len(rating_data)} rating records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bc5748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<?, ?it/s]\n",
      "100%|██████████| 1349/1349 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train indices from coverage: 2279\n",
      "Final train indices: 79429\n",
      "Train: 79429, Val: 9929, Test: 9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train/val/test splits (80/10/10)\n",
    "population = len(rating_data)\n",
    "indices = list(range(population))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Ensure each user and item appears at least once in training\n",
    "user_mention_dict = defaultdict(list)\n",
    "item_mention_dict = defaultdict(list)\n",
    "\n",
    "for i in indices:\n",
    "    user = rating_data[i]['user']\n",
    "    item = rating_data[i]['item']\n",
    "    user_mention_dict[user].append(i)\n",
    "    item_mention_dict[item].append(i)\n",
    "\n",
    "# Add at least one sample per user and item to training\n",
    "train_indices = set()\n",
    "for user, idx_list in tqdm(user_mention_dict.items()):\n",
    "    train_indices.add(random.choice(idx_list))\n",
    "for item, idx_list in tqdm(item_mention_dict.items()):\n",
    "    train_indices.add(random.choice(idx_list))\n",
    "\n",
    "print(f'Initial train indices from coverage: {len(train_indices)}')\n",
    "\n",
    "# Fill remaining to reach 80%\n",
    "remaining_indices = list(set(indices) - train_indices)\n",
    "random.shuffle(remaining_indices)\n",
    "\n",
    "train_target = int(population * 0.8)\n",
    "need_more = train_target - len(train_indices)\n",
    "train_indices.update(remaining_indices[:need_more])\n",
    "train_indices = list(train_indices)\n",
    "\n",
    "print(f'Final train indices: {len(train_indices)}')\n",
    "\n",
    "# Split remaining into val/test\n",
    "val_test_indices = list(set(indices) - set(train_indices))\n",
    "random.shuffle(val_test_indices)\n",
    "\n",
    "val_size = len(val_test_indices) // 2\n",
    "val_indices = val_test_indices[:val_size]\n",
    "test_indices = val_test_indices[val_size:]\n",
    "\n",
    "print(f'Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "063f8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rating_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create split datasets\n",
    "train_rating_data = [rating_data[i] for i in train_indices]\n",
    "val_rating_data = [rating_data[i] for i in val_indices]\n",
    "test_rating_data = [rating_data[i] for i in test_indices]\n",
    "\n",
    "rating_splits = {\n",
    "    'train': train_rating_data,\n",
    "    'val': val_rating_data,\n",
    "    'test': test_rating_data,\n",
    "    'train_indices': train_indices,\n",
    "    'val_indices': val_indices,\n",
    "    'test_indices': test_indices\n",
    "}\n",
    "\n",
    "# Save rating splits\n",
    "save_pickle(rating_splits, f'./{short_data_name}/rating_splits.pkl')\n",
    "print(f'Saved rating_splits.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d9c08",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The following files have been created in the `ml100k/` directory:\n",
    "- `sequential_data.txt`: User interaction sequences for sequential recommendation\n",
    "- `datamaps.json`: User and item ID mappings\n",
    "- `meta.json.gz`: Movie metadata (titles, genres)\n",
    "- `negative_samples.txt`: Negative samples for evaluation\n",
    "- `rating_splits.pkl`: Train/val/test splits for rating prediction\n",
    "\n",
    "You can now train P5 on this dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
